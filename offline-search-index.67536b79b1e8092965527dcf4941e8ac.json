[{"body":"Overview Veneer is a Kubernetes controller that bridges the gap between AWS cost data and Karpenter provisioning decisions. It continuously monitors Savings Plans and Reserved Instance utilization via Lumina metrics in Prometheus, then manages NodeOverlay custom resources to steer Karpenter toward cost-optimal instance types.\nData Flow flowchart LR Lumina[\"Lumina\"] Prom[\"Prometheus\"] Veneer[\"Veneer\"] NO[\"NodeOverlays\"] Karpenter[\"Karpenter\"] Fleet[\"AWS CreateFleet\"] Lumina --\u003e|\"expose SP/RI metrics\"| Prom Prom --\u003e|\"query cost data\"| Veneer Veneer --\u003e|\"create/update/delete\"| NO NO --\u003e|\"adjust pricing\"| Karpenter Karpenter --\u003e|\"Priority values\"| Fleet style Lumina fill:#e3f2fd,stroke:#1565c0,color:#1565c0 style Prom fill:#fbe9e7,stroke:#bf360c,color:#bf360c style Veneer fill:#e0f2f1,stroke:#00695c,color:#00695c style NO fill:#f1f8e9,stroke:#33691e,color:#33691e style Karpenter fill:#ede7f6,stroke:#4527a0,color:#4527a0 style Fleet fill:#fff3e0,stroke:#e65100,color:#e65100 Lumina discovers AWS Savings Plans, Reserved Instances, and running EC2 instances. It computes utilization and remaining capacity, then exposes these as Prometheus metrics. Veneer queries Prometheus on a 5-minute interval (matching Lumina’s refresh cycle). The decision engine analyzes capacity data and determines which NodeOverlays should exist. Karpenter reads NodeOverlay resources and applies price adjustments to its instance type offerings. Adjusted prices become Priority values in the AWS CreateFleet API call. AWS selects instances based on the allocation strategy and Priority values. See Instance Selection Deep Dive for details. Two Reconcilers Veneer runs two independent reconciliation loops:\nMetrics Reconciler The metrics reconciler runs on a timed interval (every 5 minutes) and is responsible for cost-aware overlay management:\nflowchart TD Start[\"Timer fires (every 5 min)\"] Fresh{\"Data fresh?\"} Query[\"Query Prometheus for\\nSP utilization, SP capacity,\\nRI counts\"] Decide{\"For each SP/RI:\\nutilization \u003c threshold\\nand capacity available?\"} Create[\"Create or update\\nNodeOverlay\"] Delete[\"Delete NodeOverlay\\n(if exists)\"] Skip[\"Skip reconciliation\\n(preserve last state)\"] Done[\"Reconciliation complete\"] Start --\u003e Fresh Fresh --\u003e|\"Yes\"| Query Fresh --\u003e|\"No — stale data\"| Skip Query --\u003e Decide Decide --\u003e|\"Yes\"| Create Decide --\u003e|\"No\"| Delete Create --\u003e Done Delete --\u003e Done Skip --\u003e Done style Start fill:#e3f2fd,stroke:#1565c0,color:#1565c0 style Fresh fill:#fff3e0,stroke:#e65100,color:#e65100 style Query fill:#e0f2f1,stroke:#00695c,color:#00695c style Decide fill:#fff3e0,stroke:#e65100,color:#e65100 style Create fill:#f1f8e9,stroke:#33691e,color:#33691e style Delete fill:#fbe9e7,stroke:#bf360c,color:#bf360c style Skip fill:#f5f5f5,stroke:#616161,color:#616161 style Done fill:#ede7f6,stroke:#4527a0,color:#4527a0 Query Prometheus for Lumina metrics: Savings Plan utilization percentages Savings Plan remaining capacity ($/hour) Reserved Instance counts by type and region Check data freshness – Skip reconciliation if Lumina data is stale Run the decision engine – For each SP and RI, determine whether a NodeOverlay should exist: Create overlay when utilization is below the threshold (default 95%) and remaining capacity exists Delete overlay when utilization exceeds the threshold or no capacity remains Apply changes – Create, update, or delete NodeOverlays in the cluster NodePool Reconciler The NodePool reconciler watches for changes to Karpenter NodePool resources and manages preference-based overlays:\nflowchart TD Watch[\"Watch NodePool changes\"] Parse[\"Parse veneer.io/preference.N\\nannotations\"] Gen[\"Generate NodeOverlay\\nfor each preference\"] Clean[\"Clean up overlays for\\nremoved preferences\"] GC[\"Garbage collect on\\nNodePool deletion\"] Watch --\u003e Parse Parse --\u003e Gen Parse --\u003e Clean Watch --\u003e|\"NodePool deleted\"| GC style Watch fill:#e3f2fd,stroke:#1565c0,color:#1565c0 style Parse fill:#e0f2f1,stroke:#00695c,color:#00695c style Gen fill:#f1f8e9,stroke:#33691e,color:#33691e style Clean fill:#fbe9e7,stroke:#bf360c,color:#bf360c style GC fill:#f5f5f5,stroke:#616161,color:#616161 Watch NodePools for veneer.io/preference.N annotations Parse preference annotations into matcher expressions and price adjustments Generate NodeOverlays for each preference Clean up overlays when preferences are removed or NodePools are deleted See Instance Preferences for annotation syntax and examples.\nOverlay Lifecycle Cost-Aware Overlays (from Lumina data) stateDiagram-v2 [*] --\u003e Active : SP below threshold\\nor RI count \u003e 0 Active --\u003e Active : Capacity still available\\n(no change) Active --\u003e Removed : Utilization exceeds threshold\\nor capacity exhausted Removed --\u003e Active : Capacity becomes available Removed --\u003e [*] Active --\u003e Preserved : Lumina data stale Preserved --\u003e Active : Fresh data arrives Preserved --\u003e Removed : Fresh data shows\\nno capacity Cost-aware overlays follow this lifecycle:\nEvent Action Overlay State SP utilization below threshold, capacity available Create overlay Active – influences Karpenter pricing SP utilization rises above threshold Delete overlay Removed – Karpenter uses default pricing RI count \u003e 0 for instance type in region Create overlay Active RI count drops to 0 Delete overlay Removed Lumina data becomes stale Skip reconciliation No change – last known state preserved Preference Overlays (from NodePool annotations) Preference overlays follow the NodePool lifecycle:\nEvent Action Preference annotation added to NodePool Create overlay Preference annotation value changed Update overlay Preference annotation removed Delete overlay NodePool deleted Overlay garbage collected via owner reference Weight Hierarchy When multiple overlays target the same instance types, the overlay with the highest weight wins:\nOverlay Type Default Weight Scope Reserved Instance 30 Instance-type specific (e.g., m5.xlarge in us-west-2) EC2 Instance Savings Plan 20 Family-specific (e.g., m5 family in us-west-2) Compute Savings Plan 10 Global (all families, all regions) Preference 1-9 (from annotation) User-defined scope Keep preference overlay weights below 10 to ensure cost-aware overlays (backed by real AWS capacity data) take precedence.\nDisabled Mode Veneer supports a “disabled” mode (overlays.disabled: true) that creates NodeOverlays with an impossible requirement (veneer.io/disabled: true). This allows testing overlay creation logic without affecting Karpenter’s provisioning decisions. The veneer_config_overlays_disabled metric reports this state.\n","categories":"","description":"Veneer's data flow, reconciliation loops, and overlay lifecycle.","excerpt":"Veneer's data flow, reconciliation loops, and overlay lifecycle.","ref":"/veneer/docs/concepts/architecture/","tags":"","title":"Architecture"},{"body":"Veneer is configured via YAML file, environment variables, or CLI flags. For Helm-based deployments, configuration values are passed through the config section of the Helm chart values. Configuration precedence (highest to lowest):\nCLI flags Environment variables (VENEER_* prefix) Configuration file values Default values Configuration File Create a config.yaml (or use config.example.yaml as a starting point):\n# Prometheus URL for querying Lumina metrics prometheusUrl: \"http://prometheus:9090\" # Log level: debug, info, warn, or error logLevel: \"info\" # Metrics endpoint bind address metricsBindAddress: \":8080\" # Health probe endpoint bind address healthProbeBindAddress: \":8081\" # AWS configuration (REQUIRED) aws: accountId: \"123456789012\" region: \"us-west-2\" # Overlay management configuration overlays: # Disabled mode: overlays created but won't match nodes disabled: false # Utilization threshold for overlay deletion (0-100) utilizationThreshold: 95.0 # Overlay priority weights weights: reservedInstance: 30 ec2InstanceSavingsPlan: 20 computeSavingsPlan: 10 # Overlay naming prefixes naming: reservedInstancePrefix: \"cost-aware-ri\" ec2InstanceSavingsPlanPrefix: \"cost-aware-ec2-sp\" computeSavingsPlanPrefix: \"cost-aware-compute-sp\" # Instance preference configuration preferences: enabled: true All Configuration Options Core Settings Option YAML Key Env Variable Default Description Prometheus URL prometheusUrl VENEER_PROMETHEUS_URL http://prometheus:9090 URL of the Prometheus server for Lumina metrics Log Level logLevel VENEER_LOG_LEVEL info Log verbosity: debug, info, warn, error Metrics Bind Address metricsBindAddress VENEER_METRICS_BIND_ADDRESS :8080 Address for the Prometheus metrics endpoint Health Probe Bind Address healthProbeBindAddress VENEER_HEALTH_PROBE_BIND_ADDRESS :8081 Address for health and readiness probes AWS Settings (Required) Option YAML Key Env Variable Default Description Account ID aws.accountId VENEER_AWS_ACCOUNT_ID (none) 12-digit AWS account ID where this cluster runs Region aws.region VENEER_AWS_REGION (none) AWS region where this cluster runs Both aws.accountId and aws.region are required. Veneer uses them to scope Prometheus queries to only return RI/SP data from this specific account and region.\nOverlay Management Option YAML Key Env Variable Default Description Disabled Mode overlays.disabled VENEER_OVERLAY_DISABLED false When true, overlays are created with an impossible requirement so they never match Utilization Threshold overlays.utilizationThreshold – 95.0 SP/RI utilization percentage at which overlays are deleted (0-100) Overlay Weights Weights control overlay precedence when multiple overlays target the same instances. Higher weight wins. See the NodeOverlay CRD reference for details on the weight system.\nOption YAML Key Default Description Reserved Instance Weight overlays.weights.reservedInstance 30 Weight for RI-backed overlays (instance-type specific) EC2 Instance SP Weight overlays.weights.ec2InstanceSavingsPlan 20 Weight for EC2 Instance SP overlays (family-specific) Compute SP Weight overlays.weights.computeSavingsPlan 10 Weight for Compute SP overlays (global) Overlay Naming Option YAML Key Default Description RI Prefix overlays.naming.reservedInstancePrefix cost-aware-ri Name prefix for RI overlays (e.g., cost-aware-ri-m5-xlarge-us-west-2) EC2 Instance SP Prefix overlays.naming.ec2InstanceSavingsPlanPrefix cost-aware-ec2-sp Name prefix for EC2 Instance SP overlays Compute SP Prefix overlays.naming.computeSavingsPlanPrefix cost-aware-compute-sp Name prefix for Compute SP overlays Instance Preferences Option YAML Key Default Description Enabled preferences.enabled true Whether to process veneer.io/preference.N annotations on NodePools Environment Variables All core settings can be overridden via environment variables:\nexport VENEER_PROMETHEUS_URL=\"http://prometheus.example.com:9090\" export VENEER_LOG_LEVEL=\"debug\" export VENEER_METRICS_BIND_ADDRESS=\":9090\" export VENEER_HEALTH_PROBE_BIND_ADDRESS=\":9091\" export VENEER_AWS_ACCOUNT_ID=\"123456789012\" export VENEER_AWS_REGION=\"us-west-2\" export VENEER_OVERLAY_DISABLED=\"true\" CLI Flags Command-line flags override both config file and environment variables:\n./bin/manager --config=config.yaml --overlay-disabled ./bin/manager --help # View all available flags Local Development Configuration For local development with kubectl port-forward:\n# config.local.yaml prometheusUrl: \"http://localhost:9090\" logLevel: \"debug\" metricsBindAddress: \":8080\" healthProbeBindAddress: \":8081\" aws: accountId: \"123456789012\" region: \"us-west-2\" overlays: disabled: false # Port-forward to Prometheus kubectl port-forward -n lumina-system svc/lumina-prometheus 9090:9090 # Run with local config make run Validation Veneer validates configuration at startup:\nprometheusUrl must be non-empty aws.accountId must be exactly 12 digits aws.region must be non-empty logLevel must be one of: debug, info, warn, error overlays.utilizationThreshold must be between 0 and 100 All overlay weights must be non-negative ","categories":"","description":"All Veneer configuration options, environment variables, CLI flags, and defaults.","excerpt":"All Veneer configuration options, environment variables, CLI flags, …","ref":"/veneer/docs/reference/configuration/","tags":"","title":"Configuration"},{"body":"Before installing Veneer, ensure the following prerequisites are in place:\nKarpenter (v0.32+) – Veneer manages NodeOverlay resources that Karpenter consumes for instance selection decisions. Lumina – Provides the AWS Savings Plan and Reserved Instance cost metrics that Veneer queries via Prometheus. See the Lumina documentation for setup instructions. Prometheus – Lumina exposes its metrics to Prometheus, and Veneer queries Prometheus to retrieve them. Any Prometheus-compatible server works. The installation guide walks you through deploying Veneer via Helm, configuring it to connect to your Prometheus instance, and verifying that NodeOverlays are being created correctly.\n","categories":"","description":"Install and configure Veneer for cost-aware Karpenter provisioning.","excerpt":"Install and configure Veneer for cost-aware Karpenter provisioning.","ref":"/veneer/docs/getting-started/","tags":"","title":"Getting Started"},{"body":"Prerequisites Veneer requires the following components in your Kubernetes cluster:\nComponent Version Purpose Karpenter v1.0+ (with NodeOverlay support) Node provisioning with NodeOverlay CRD Lumina Latest Exposes RI/SP metrics to Prometheus Prometheus Any Scrapes and stores Lumina metrics Karpenter with NodeOverlay Support Veneer manages NodeOverlay custom resources, which are part of the Karpenter v1alpha1 API group. Ensure your Karpenter installation includes the NodeOverlay CRD:\nkubectl get crd nodeoverlays.karpenter.sh Lumina Lumina must be deployed and actively exposing Savings Plans and Reserved Instance metrics. Verify Lumina is running:\nkubectl get pods -n lumina-system Prometheus A Prometheus server must be scraping Lumina metrics. Veneer queries Prometheus for:\nsavings_plan_remaining_capacity – SP remaining capacity in dollars/hour savings_plan_utilization_percent – SP utilization percentage ec2_reserved_instance – Reserved Instance counts by type Verify metrics are available:\ncurl -s 'http://\u003cprometheus-url\u003e:9090/api/v1/query?query=savings_plan_remaining_capacity' | jq '.data.result | length' Helm Installation Add the Helm Repository helm repo add veneer https://nextdoor.github.io/veneer helm repo update Install Veneer helm install veneer veneer/veneer \\ --namespace veneer-system \\ --create-namespace \\ --set config.prometheusUrl=http://lumina-prometheus.lumina-system.svc:9090 \\ --set config.aws.accountId=123456789012 \\ --set config.aws.region=us-west-2 The config.aws.accountId and config.aws.region values are required. Veneer uses them to scope Prometheus queries to only return capacity data relevant to this cluster.\nInstall from Local Chart If installing from the repository source:\nhelm install veneer ./charts/veneer \\ --namespace veneer-system \\ --create-namespace \\ --set config.prometheusUrl=http://lumina-prometheus.lumina-system.svc:9090 \\ --set config.aws.accountId=123456789012 \\ --set config.aws.region=us-west-2 Custom Values File For production deployments, create a values.yaml:\nreplicaCount: 2 config: prometheusUrl: \"http://lumina-prometheus.lumina-system.svc:9090\" logLevel: \"info\" aws: accountId: \"123456789012\" region: \"us-west-2\" overlays: utilizationThreshold: 95.0 weights: reservedInstance: 30 ec2InstanceSavingsPlan: 20 computeSavingsPlan: 10 serviceMonitor: enabled: true helm install veneer veneer/veneer \\ --namespace veneer-system \\ --create-namespace \\ -f values.yaml See the Helm Chart Reference for all available values.\nVerification Check Pod Status kubectl get pods -n veneer-system You should see the Veneer controller pods running (2 replicas by default with leader election):\nNAME READY STATUS RESTARTS AGE veneer-6b8f9c4d7f-abc12 1/1 Running 0 2m veneer-6b8f9c4d7f-def34 1/1 Running 0 2m Check Health Endpoints # Port-forward to the controller kubectl port-forward -n veneer-system svc/veneer 8081:8081 # Check health curl http://localhost:8081/healthz # Expected: ok # Check readiness curl http://localhost:8081/readyz # Expected: ok Check Metrics kubectl port-forward -n veneer-system svc/veneer-metrics 8080:8080 curl -s http://localhost:8080/metrics | grep veneer_ You should see metrics including veneer_info, veneer_lumina_data_available, and others. See Metrics Reference for the full catalog.\nVerify NodeOverlay Creation After Veneer has run at least one reconciliation cycle (default 5 minutes), check for NodeOverlays:\nkubectl get nodeoverlays -l app.kubernetes.io/managed-by=veneer If Lumina reports available Savings Plans or Reserved Instance capacity, you should see overlays created with names like cost-aware-ri-*, cost-aware-ec2-sp-*, or cost-aware-compute-sp-*.\nCheck Logs kubectl logs -n veneer-system -l app.kubernetes.io/name=veneer --tail=50 Look for log lines indicating successful reconciliation:\nINFO metrics-reconciler Starting metrics reconciler {\"interval\": \"5m\"} INFO metrics-reconciler Reconciliation complete {\"overlays-created\": 3, \"overlays-deleted\": 0} ","categories":"","description":"Prerequisites, Helm installation, and verification for Veneer.","excerpt":"Prerequisites, Helm installation, and verification for Veneer.","ref":"/veneer/docs/getting-started/installation/","tags":"","title":"Installation"},{"body":"Veneer bridges the gap between Lumina’s real-time cost data and Karpenter’s provisioning decisions. It watches Prometheus for Savings Plan and Reserved Instance utilization metrics, then creates and manages NodeOverlay custom resources that adjust Karpenter’s effective instance pricing. This steers Karpenter toward cost-optimal instance types without replacing its core scheduling and bin-packing logic.\nThe key mechanism is the NodeOverlay CRD: each overlay targets a set of instance types via label requirements and applies a price adjustment. Karpenter uses these adjusted prices as Priority values in the AWS CreateFleet API, causing AWS to prefer the instances Veneer has identified as cost-effective.\nThe pages in this section cover:\nArchitecture – The end-to-end data flow from Lumina through Veneer to Karpenter, the two reconciliation loops (Metrics Reconciler and NodePool Reconciler), and the overlay lifecycle. Instance Selection Deep Dive – How Karpenter translates NodeOverlay price adjustments into AWS CreateFleet Priority values, and how allocation strategies affect the final instance choice. Bin-Packing and NodeOverlay – How Karpenter’s bin-packing step can filter out instance types before NodeOverlay price adjustments take effect, and what that means for provisioning outcomes. Instance Preferences – How to use veneer.io/preference.N annotations on NodePools to express instance type preferences independent of cost data. ","categories":"","description":"Core concepts behind Veneer's cost-aware provisioning approach.","excerpt":"Core concepts behind Veneer's cost-aware provisioning approach.","ref":"/veneer/docs/concepts/","tags":"","title":"Concepts"},{"body":"Veneer is a Kubernetes controller that optimizes Karpenter provisioning decisions by managing NodeOverlay resources based on real-time AWS Reserved Instance and Savings Plans data from Lumina.\nHow It Works flowchart TD subgraph AWS[\"AWS\"] direction LR RI[\"Reserved Instances\"] SP[\"Savings Plans\"] EC2[\"EC2 Instances\"] end Lumina[\"Lumina — Cost Data Controller\"] Prom[\"Prometheus\"] Veneer[\"Veneer — Overlay Controller\"] NO[\"NodeOverlays\"] Karpenter[\"Karpenter\"] Nodes[\"Cost-Optimized EC2 Nodes\"] RI \u0026 SP \u0026 EC2 --\u003e Lumina Lumina --\u003e|\"expose RI/SP cost metrics\"| Prom Prom --\u003e|\"query instance cost data\"| Veneer Veneer --\u003e|\"create \u0026 update overlays\"| NO NO --\u003e|\"adjust instance pricing \u0026 priority\"| Karpenter Karpenter --\u003e|\"provision nodes\"| Nodes style AWS fill:#fff3e0,stroke:#e65100,color:#e65100 style Lumina fill:#e3f2fd,stroke:#1565c0,color:#1565c0 style Prom fill:#fbe9e7,stroke:#bf360c,color:#bf360c style Veneer fill:#e0f2f1,stroke:#00695c,color:#00695c style NO fill:#f1f8e9,stroke:#33691e,color:#33691e style Karpenter fill:#ede7f6,stroke:#4527a0,color:#4527a0 style Nodes fill:#f5f5f5,stroke:#616161,color:#616161 style RI fill:#fff3e0,stroke:#e65100,color:#e65100 style SP fill:#fff3e0,stroke:#e65100,color:#e65100 style EC2 fill:#fff3e0,stroke:#e65100,color:#e65100 Veneer continuously watches Lumina’s cost metrics and creates/updates/deletes Karpenter NodeOverlay CRs to prefer RI/SP-covered on-demand instances when cost-effective, fall back to spot when capacity is exhausted, avoid provisioning thrashing with smart debouncing, and express instance preferences via NodePool annotations.\nUse the section navigation below to explore the documentation.\n","categories":"","description":"Comprehensive documentation for the Veneer cost-aware Karpenter provisioning controller.","excerpt":"Comprehensive documentation for the Veneer cost-aware Karpenter …","ref":"/veneer/docs/","tags":"","title":"Documentation"},{"body":"This document provides a detailed technical explanation of how Karpenter selects EC2 instances, from pod scheduling through the AWS CreateFleet API call. Understanding this flow is essential for configuring Veneer’s NodeOverlay feature to influence instance selection.\nOverview Karpenter does not make the final instance type selection. Instead, it:\nFilters instance types based on NodePool requirements and pod constraints Sorts instance types by adjusted price Truncates to a maximum of 60 instance types Delegates the final selection to AWS via the CreateFleet API AWS CreateFleet makes the ultimate decision based on:\nSpot capacity availability Allocation strategy Priority values (when using capacity-optimized-prioritized) Architecture Flow flowchart TB subgraph KARPENTER[\"KARPENTER CONTROLLER\"] direction TB subgraph Phase1[\"Phase 1: Scheduling\"] Scheduler[\"Scheduler\u003cbr/\u003e(Bin-pack)\"] AllTypes[\"ALL instance\u003cbr/\u003etypes kept\"] Scheduler --\u003e AllTypes end subgraph Phase2[\"Phase 2: Launch Templates\"] Resolver[\"Launch Template\u003cbr/\u003eResolver\"] GroupAMI[\"Group by AMI\u003cbr/\u003e(architecture)\"] Resolver --\u003e GroupAMI end subgraph Phase3[\"Phase 3: Fleet Request\"] Builder[\"CreateFleet\u003cbr/\u003eBuilder\"] Priority[\"Set Priority values\u003cbr/\u003e(adjusted prices)\"] Builder --\u003e Priority end Phase1 --\u003e Phase2 --\u003e Phase3 end subgraph AWS[\"AWS CreateFleet API\"] direction TB Step1[\"1. Check spot capacity for each\u003cbr/\u003einstance type/AZ combination\"] Step2[\"2. Apply allocation strategy\u003cbr/\u003e(price-capacity-optimized OR\u003cbr/\u003ecapacity-optimized-prioritized)\"] Step3[\"3. Select instance type\u003cbr/\u003ewith best score\"] Step1 --\u003e Step2 --\u003e Step3 end KARPENTER --\u003e AWS Phase 1: Scheduling and Bin-Packing Code Location Repository: kubernetes-sigs/karpenter File: pkg/controllers/provisioning/scheduling/scheduler.go What Happens The scheduler performs bin-packing simulation to determine which pods can fit on potential nodes. Critically, it does not select a specific instance type or architecture at this stage.\nSource: scheduler.go#L103-L150\n// The scheduler keeps ALL compatible instance types - it does not pick ARM vs x86 here func (s *Scheduler) Solve(ctx context.Context, pods []*corev1.Pod) Results { // ... bin-packing logic ... // Each NodeClaim contains ALL instance types that could satisfy the requirements } Key Points No architecture decision: The scheduler doesn’t choose between ARM64 and x86 All compatible types kept: If 100 instance types match requirements, all 100 are passed forward Filtering only: NodePool requirements filter out incompatible types, but don’t select specific ones Phase 2: Launch Template Generation Code Location Repository: aws/karpenter-provider-aws File: pkg/providers/amifamily/resolver.go AMI-Based Grouping Since AMIs are architecture-specific (ARM64 vs x86), Karpenter creates separate launch template configurations for each AMI:\nSource: resolver.go#L145-L196\nfunc (r DefaultResolver) Resolve(nodeClass *v1.EC2NodeClass, nodeClaim *karpv1.NodeClaim, instanceTypes []*cloudprovider.InstanceType, capacityType string, ...) ([]*LaunchTemplate, error) { // Map AMIs to compatible instance types mappedAMIs := MapToInstanceTypes(instanceTypes, nodeClass.Status.AMIs) var resolvedTemplates []*LaunchTemplate for amiID, instanceTypes := range mappedAMIs { // Each AMI (architecture) gets its own launch template config // ARM64 instances -\u003e ARM64 AMI launch template // x86 instances -\u003e x86 AMI launch template } return resolvedTemplates, nil } MapToInstanceTypes Function Source: ami.go#L228-L241\nfunc MapToInstanceTypes(instanceTypes []*cloudprovider.InstanceType, amis []v1.AMI) map[string][]*cloudprovider.InstanceType { amiIDs := map[string][]*cloudprovider.InstanceType{} for _, instanceType := range instanceTypes { for _, ami := range amis { if err := instanceType.Requirements.Compatible( scheduling.NewNodeSelectorRequirements(ami.Requirements...), ); err == nil { amiIDs[ami.ID] = append(amiIDs[ami.ID], instanceType) break } } } return amiIDs } Phase 3: CreateFleet API Call Code Location Repository: aws/karpenter-provider-aws File: pkg/providers/instance/instance.go Instance Type Limit Karpenter enforces a maximum of 60 instance types per CreateFleet request:\nSource: instance.go#L63\nconst maxInstanceTypes = 60 Sorting and Truncation Before sending to AWS, instance types are sorted by price and truncated:\nSource: types.go#L221-L233\nfunc (its InstanceTypes) OrderByPrice(reqs scheduling.Requirements) InstanceTypes { sort.Slice(its, func(i, j int) bool { iPrice := its[i].Offerings.Available().CompatibleFor(reqs).LowestPrice() jPrice := its[j].Offerings.Available().CompatibleFor(reqs).LowestPrice() if iPrice != jPrice { return iPrice \u003c jPrice } return its[i].Name \u003c its[j].Name }) return its } Source: types.go#L322-L327\nfunc (its InstanceTypes) Truncate(reqs scheduling.Requirements, maxItems int) InstanceTypes { its = its.OrderByPrice(reqs) if len(its) \u003e maxItems { return its[:maxItems] } return its } Building the CreateFleet Request Source: instance.go#L456-L486\nfunc (p *DefaultProvider) getOverrides( instanceTypes []*cloudprovider.InstanceType, offerings cloudprovider.Offerings, zones, subnets *resourceSet, image string, ) []ec2types.FleetLaunchTemplateOverridesRequest { var overrides []ec2types.FleetLaunchTemplateOverridesRequest for _, offering := range offerings { // ... zone/subnet matching ... overrides = append(overrides, ec2types.FleetLaunchTemplateOverridesRequest{ InstanceType: ec2types.InstanceType(instanceType.Name), SubnetId: aws.String(subnet), ImageId: aws.String(image), AvailabilityZone: aws.String(offering.Requirements.Get(karpv1.TopologyLabelZone).Any()), // CRITICAL: Priority is set to the offering price Priority: lo.ToPtr(float64(offering.Price)), }) } return overrides } Allocation Strategies Code Location Repository: aws/karpenter-provider-aws File: pkg/providers/instance/types.go#L209-L240 Strategy Selection Logic Source: types.go#L209-L240\nfunc (b *CreateFleetInputBuilder) Build() *ec2.CreateFleetInput { input := \u0026ec2.CreateFleetInput{ Type: ec2types.FleetTypeInstant, // ... } if b.capacityType == karpv1.CapacityTypeSpot { input.SpotOptions = \u0026ec2types.SpotOptionsRequest{ // WITH NodeOverlay (overlay=true): capacity-optimized-prioritized // WITHOUT NodeOverlay (overlay=false): price-capacity-optimized AllocationStrategy: lo.Ternary( b.overlay, ec2types.SpotAllocationStrategyCapacityOptimizedPrioritized, ec2types.SpotAllocationStrategyPriceCapacityOptimized, ), } } else if b.capacityReservationType != v1.CapacityReservationTypeCapacityBlock { input.OnDemandOptions = \u0026ec2types.OnDemandOptionsRequest{ // WITH NodeOverlay: prioritized // WITHOUT NodeOverlay: lowest-price AllocationStrategy: lo.Ternary( b.overlay, ec2types.FleetOnDemandAllocationStrategyPrioritized, ec2types.FleetOnDemandAllocationStrategyLowestPrice, ), } } return input } Strategy Comparison Scenario Spot Strategy On-Demand Strategy Default (no NodeOverlay) price-capacity-optimized lowest-price With NodeOverlay capacity-optimized-prioritized prioritized How Each Strategy Works AWS Documentation Reference: The FleetLaunchTemplateOverridesRequest.Priority documentation explicitly states when Priority is used:\n“If the Spot AllocationStrategy is set to capacity-optimized-prioritized, EC2 Fleet uses priority on a best-effort basis to determine which launch template override to use in fulfilling Spot capacity, but optimizes for capacity first.”\n“If the On-Demand AllocationStrategy is set to prioritized, EC2 Fleet uses priority to determine which launch template override to use first in fulfilling On-Demand capacity.”\nThis means Priority is only used with these two specific strategies.\nprice-capacity-optimized (Default for Spot) AWS balances price and capacity availability Automatically diversifies across pools to reduce interruptions Does not use Priority values – the Priority field is ignored even if set capacity-optimized-prioritized (With NodeOverlay) AWS first ensures capacity is available Among available pools, selects based on Priority (lower = better) Uses Priority values – this is how Veneer influences selection lowest-price (Default for On-Demand) Selects the cheapest instance type Does not use Priority values – selection is purely price-based prioritized (With NodeOverlay for On-Demand) Selects based on Priority (lower = better) Uses Priority values Priority Values and Price Adjustments How Veneer Adjustments Work When a NodeOverlay specifies a price adjustment (e.g., adjust=-50%), Karpenter modifies the offering price before setting the Priority:\nSource: types.go#L369-L384\nfunc (o Offering) AdjustedPrice() float64 { // If no overlay, return base price if o.Overlay == nil || o.Overlay.Adjustment == nil { return o.Price } // Apply percentage adjustment if o.Overlay.Adjustment.Percentage != nil { return o.Price * (1 + *o.Overlay.Adjustment.Percentage/100) } // Apply absolute adjustment if o.Overlay.Adjustment.Absolute != nil { return o.Price + *o.Overlay.Adjustment.Absolute } return o.Price } Example: ARM64 -50% Adjustment Given base spot prices:\nm8g.24xlarge (ARM64): $1.27/hr m7i.12xlarge (x86): $0.78/hr With -50% adjustment on ARM64:\nm8g.24xlarge adjusted: $1.27 * 0.5 = $0.635/hr m7i.12xlarge unchanged: $0.78/hr Priority values sent to CreateFleet:\nm8g.24xlarge: Priority = 0.635 (lower = better) m7i.12xlarge: Priority = 0.78 How AWS Makes the Final Selection With capacity-optimized-prioritized flowchart TB subgraph AWS[\"AWS CreateFleet Decision Process\"] direction TB subgraph Step1[\"Step 1: Check Spot Capacity\"] Check[\"For each instance type/AZ combination:\"] Query[\"Query spot capacity pools\"] Eliminate[\"Eliminate types with insufficient capacity\"] Check --\u003e Query --\u003e Eliminate end subgraph Step2[\"Step 2: Apply Priority Ranking\"] Among[\"Among instance types WITH available capacity:\"] Sort[\"Sort by Priority value (ascending)\"] Lower[\"Lower Priority = Higher preference\"] Among --\u003e Sort --\u003e Lower end subgraph Step3[\"Step 3: Select Winner\"] Launch[\"Launch instance from pool with:\"] Available[\"Available capacity (mandatory)\"] Lowest[\"Lowest Priority value (when capacity available)\"] Launch --\u003e Available --\u003e Lowest end Step1 --\u003e Step2 --\u003e Step3 end Step3 --\u003e Result[\"Selected Instance\"] style Result fill:#90EE90 Why x86 Might Still Be Selected Even with lower Priority values for ARM64, AWS may select x86 if:\nARM64 spot pools are exhausted – No capacity available ARM64 has recent interruptions – AWS may deprioritize volatile pools AZ constraints – Required AZ only has x86 capacity Real-World Examples Example CreateFleet Request (with NodeOverlay) { \"Type\": \"instant\", \"TargetCapacitySpecification\": { \"DefaultTargetCapacityType\": \"spot\", \"TotalTargetCapacity\": 1 }, \"SpotOptions\": { \"AllocationStrategy\": \"capacity-optimized-prioritized\" }, \"LaunchTemplateConfigs\": [ { \"LaunchTemplateSpecification\": { \"LaunchTemplateName\": \"karpenter.k8s.aws/13299866299363256344\", \"Version\": \"$Latest\" }, \"Overrides\": [ { \"InstanceType\": \"c6gn.16xlarge\", \"ImageId\": \"ami-06bd99f6c8a066836\", \"AvailabilityZone\": \"us-west-2b\", \"SubnetId\": \"subnet-0c7dc4bd1e242e84b\", \"Priority\": 0.29118 }, { \"InstanceType\": \"m8g.12xlarge\", \"ImageId\": \"ami-06bd99f6c8a066836\", \"AvailabilityZone\": \"us-west-2b\", \"SubnetId\": \"subnet-0c7dc4bd1e242e84b\", \"Priority\": 0.49020 }, { \"InstanceType\": \"m8g.24xlarge\", \"ImageId\": \"ami-06bd99f6c8a066836\", \"AvailabilityZone\": \"us-west-2b\", \"SubnetId\": \"subnet-0c7dc4bd1e242e84b\", \"Priority\": 0.63342 } ] }, { \"LaunchTemplateSpecification\": { \"LaunchTemplateName\": \"karpenter.k8s.aws/2908816957350676553\", \"Version\": \"$Latest\" }, \"Overrides\": [ { \"InstanceType\": \"m7i.12xlarge\", \"ImageId\": \"ami-0894e3f68fa1384c4\", \"AvailabilityZone\": \"us-west-2b\", \"SubnetId\": \"subnet-0c7dc4bd1e242e84b\", \"Priority\": 0.7807 }, { \"InstanceType\": \"m6i.12xlarge\", \"ImageId\": \"ami-0894e3f68fa1384c4\", \"AvailabilityZone\": \"us-west-2b\", \"SubnetId\": \"subnet-0c7dc4bd1e242e84b\", \"Priority\": 0.8202 } ] } ] } Key observations:\nTwo launch template configs: one for ARM64 AMI, one for x86 AMI ARM64 instances have lower Priority values (0.29 - 0.63) due to -50% adjustment x86 instances have higher Priority values (0.78 - 0.82) Example CreateFleet Response { \"fleetId\": \"fleet-989ca286-f32f-ce94-043a-05a885465b32\", \"fleetInstanceSet\": { \"item\": { \"lifecycle\": \"spot\", \"instanceType\": \"m8g.24xlarge\", \"instanceIds\": { \"item\": \"i-0708bc0ab5c887271\" }, \"launchTemplateAndOverrides\": { \"overrides\": { \"instanceType\": \"m8g.24xlarge\", \"imageId\": \"ami-06bd99f6c8a066836\", \"availabilityZone\": \"us-west-2b\", \"priority\": 0.63342 } } } } } Result: AWS selected m8g.24xlarge (ARM64) because:\nSpot capacity was available for this instance type It had a lower Priority than x86 alternatives (0.63 vs 0.78+) Example: When x86 Gets Selected Despite Lower ARM64 Priority { \"fleetInstanceSet\": { \"item\": { \"lifecycle\": \"spot\", \"instanceType\": \"m7a.32xlarge\", \"launchTemplateAndOverrides\": { \"overrides\": { \"instanceType\": \"m7a.32xlarge\", \"priority\": 0.95 } } } }, \"errorSet\": { \"item\": [ { \"errorCode\": \"InsufficientInstanceCapacity\", \"errorMessage\": \"We currently do not have sufficient m8g.24xlarge capacity...\", \"launchTemplateAndOverrides\": { \"overrides\": { \"instanceType\": \"m8g.24xlarge\", \"priority\": 0.63 } } } ] } } This response shows that even though m8g.24xlarge had a better Priority (0.63), AWS selected m7a.32xlarge (0.95) because the ARM64 pool had insufficient capacity.\nImplications for Veneer NodeOverlay How NodeOverlay Influences Selection Triggers capacity-optimized-prioritized strategy – Without NodeOverlay, Karpenter uses price-capacity-optimized which ignores Priority values entirely\nSets Priority values – The adjusted prices become Priority values in the CreateFleet request\nDoes NOT guarantee selection – AWS capacity availability takes precedence\nEffective Configuration apiVersion: karpenter.sh/v1 kind: NodePool metadata: name: my-nodepool annotations: # Veneer annotation to prefer ARM64 with -50% price adjustment veneer.io/preference.1: \"kubernetes.io/arch=arm64 adjust=-50%\" spec: template: spec: requirements: # Allow both architectures - key: kubernetes.io/arch operator: In values: [\"amd64\", \"arm64\"] What This Achieves Without NodeOverlay With NodeOverlay (-50% ARM64) Strategy: price-capacity-optimized Strategy: capacity-optimized-prioritized AWS balances price + capacity AWS prioritizes capacity, then Priority Priority values ignored Priority values used for selection ~50/50 ARM64/x86 selection ARM64 preferred when capacity available Code References Component Repository File Line Max instance types karpenter-provider-aws pkg/providers/instance/instance.go 63 Priority assignment karpenter-provider-aws pkg/providers/instance/instance.go 476 Allocation strategy karpenter-provider-aws pkg/providers/instance/types.go 209-217 Price adjustment karpenter pkg/cloudprovider/types.go 369-384 Instance type sorting karpenter pkg/cloudprovider/types.go 221-233 Instance type truncation karpenter pkg/cloudprovider/types.go 322-327 AMI to instance mapping karpenter-provider-aws pkg/providers/amifamily/ami.go 228-241 Launch template resolution karpenter-provider-aws pkg/providers/amifamily/resolver.go 135-197 Summary Karpenter doesn’t select the instance – It filters, sorts, and sends options to AWS NodeOverlay enables Priority-based selection – Changes allocation strategy from price-capacity-optimized to capacity-optimized-prioritized Priority = Adjusted Price – Lower values = higher preference Capacity trumps Priority – AWS will select a higher-Priority (worse) instance if the lower-Priority option lacks capacity Architecture is grouped by AMI – ARM64 and x86 instances use different launch templates but are sent in the same CreateFleet request Maximum 60 instance types – Karpenter truncates the list after sorting by price ","categories":"","description":"Technical deep dive into how Karpenter selects EC2 instances, from pod scheduling through the AWS CreateFleet API call.","excerpt":"Technical deep dive into how Karpenter selects EC2 instances, from pod …","ref":"/veneer/docs/concepts/instance-selection/","tags":"","title":"Instance Selection Deep Dive"},{"body":"Veneer exposes Prometheus metrics on the metrics endpoint (default :8080/metrics, configurable via metricsBindAddress). All metrics use the veneer_ namespace prefix.\nVeneer intentionally does not duplicate Lumina metrics (which are already in Prometheus). Instead, it focuses on what Veneer decided and what actions it took.\nMetrics at a Glance Metric Type Description veneer_reconciliation_duration_seconds Histogram Duration of reconciliation cycles veneer_reconciliation_total Counter Total reconciliation cycles veneer_lumina_data_freshness_seconds Gauge Age of Lumina data veneer_lumina_data_available Gauge Whether Lumina data is fresh veneer_decision_total Counter Decisions made by the engine veneer_reserved_instance_data_available Gauge Whether RI metrics are available veneer_reserved_instance_count Gauge RI count by type and region veneer_savings_plan_utilization_percent Gauge SP utilization percentage veneer_savings_plan_remaining_capacity_dollars Gauge SP remaining capacity ($/hr) veneer_overlay_operations_total Counter Total overlay operations veneer_overlay_operation_errors_total Counter Total overlay operation errors veneer_overlay_count Gauge Current overlay count veneer_prometheus_query_duration_seconds Histogram Prometheus query duration veneer_prometheus_query_errors_total Counter Prometheus query errors veneer_prometheus_query_result_count Gauge Prometheus query result count veneer_config_overlays_disabled Gauge Whether overlays are disabled veneer_config_utilization_threshold_percent Gauge Configured utilization threshold veneer_info Gauge Controller version info Reconciliation Metrics Metric Type Labels Description veneer_reconciliation_duration_seconds Histogram – Duration of metrics reconciliation cycles. Buckets: 0.1s to ~51s (exponential). veneer_reconciliation_total Counter result Total number of reconciliation cycles. Labels: result=success|error. Data Source Health Metrics Metric Type Labels Description veneer_lumina_data_freshness_seconds Gauge – Age of Lumina data in seconds. veneer_lumina_data_available Gauge – 1 if Lumina data is available and fresh, 0 if stale or unavailable. Decision Metrics Metric Type Labels Description veneer_decision_total Counter capacity_type, should_exist, reason Total decisions made by the decision engine. Label values for veneer_decision_total:\nLabel Values Description capacity_type compute_savings_plan, ec2_instance_savings_plan, reserved_instance, preference Type of AWS pre-paid capacity should_exist true, false Whether an overlay should exist based on the decision reason capacity_available, utilization_above_threshold, no_capacity, ri_available, ri_not_found, unknown Reason for the decision Reserved Instance Metrics Metric Type Labels Description veneer_reserved_instance_data_available Gauge – 1 if Lumina is exposing RI metrics, 0 if not. veneer_reserved_instance_count Gauge instance_type, region Number of Reserved Instances detected by instance type and region. Savings Plan Metrics Metric Type Labels Description veneer_savings_plan_utilization_percent Gauge type, instance_family, region Savings Plan utilization percentage. veneer_savings_plan_remaining_capacity_dollars Gauge type, instance_family, region Savings Plan remaining capacity in dollars per hour. Label values:\nLabel Values Description type SP type identifier Type of Savings Plan instance_family Family name or all Instance family (or all for Compute SPs) region AWS region or global Region scope NodeOverlay Lifecycle Metrics Metric Type Labels Description veneer_overlay_operations_total Counter operation, capacity_type Total NodeOverlay operations. veneer_overlay_operation_errors_total Counter operation, error_type Total NodeOverlay operation errors. veneer_overlay_count Gauge capacity_type Current number of NodeOverlays managed by Veneer. Label values:\nLabel Values Description operation create, update, delete Type of overlay operation capacity_type compute_savings_plan, ec2_instance_savings_plan, reserved_instance, preference Capacity type the overlay targets error_type validation, api, not_found Type of error encountered Prometheus Query Metrics Metric Type Labels Description veneer_prometheus_query_duration_seconds Histogram query_type Duration of Prometheus queries to Lumina. Uses default Prometheus buckets. veneer_prometheus_query_errors_total Counter query_type Total Prometheus query errors. veneer_prometheus_query_result_count Gauge query_type Number of results returned by the last Prometheus query. Label values for query_type:\nValue Description sp_utilization Savings Plan utilization query sp_capacity Savings Plan remaining capacity query ri Reserved Instance count query data_freshness Lumina data freshness check Configuration Metrics Metric Type Labels Description veneer_config_overlays_disabled Gauge – 1 if overlay creation is disabled (dry-run mode), 0 if enabled. veneer_config_utilization_threshold_percent Gauge – Configured utilization threshold for overlay deletion. Info Metric Metric Type Labels Description veneer_info Gauge version, disabled_mode Controller information. Always set to 1. Example PromQL Queries Reconciliation Health # Reconciliation error rate (last 5 minutes) rate(veneer_reconciliation_total{result=\"error\"}[5m]) / rate(veneer_reconciliation_total[5m]) # Average reconciliation duration rate(veneer_reconciliation_duration_seconds_sum[5m]) / rate(veneer_reconciliation_duration_seconds_count[5m]) Data Source Health # Alert if Lumina data is unavailable veneer_lumina_data_available == 0 # Data freshness in minutes veneer_lumina_data_freshness_seconds / 60 Overlay Activity # Overlay creation rate by capacity type rate(veneer_overlay_operations_total{operation=\"create\"}[1h]) # Current overlay count veneer_overlay_count # Overlay operation error rate rate(veneer_overlay_operation_errors_total[5m]) Savings Plans Monitoring # SP utilization across all types veneer_savings_plan_utilization_percent # Remaining SP capacity ($/hour) veneer_savings_plan_remaining_capacity_dollars Grafana Dashboard You can build a Grafana dashboard using these metrics. Key panels to include:\nReconciliation Status – Success/error rate over time Lumina Data Freshness – Gauge showing data age Overlay Count – Breakdown by capacity type Decision Activity – Create vs delete decisions over time Prometheus Query Performance – Query latency and error rates SP Utilization – Per-type utilization percentages ","categories":"","description":"Prometheus metrics exposed by the Veneer controller.","excerpt":"Prometheus metrics exposed by the Veneer controller.","ref":"/veneer/docs/reference/metrics/","tags":"","title":"Metrics"},{"body":"This document explains how Karpenter’s bin-packing algorithm can affect – and sometimes bypass – NodeOverlay price adjustments, leading to unexpected instance selection behavior.\nOverview Veneer’s NodeOverlay feature influences instance selection by adjusting prices, which become Priority values in AWS CreateFleet requests. However, this influence only works when multiple instance types are eligible candidates.\nThe key insight: Karpenter’s bin-packing algorithm filters instance types before NodeOverlay can influence selection. If bin-packing eliminates all instances of a particular architecture, NodeOverlay has nothing to prefer.\nflowchart LR subgraph Karpenter[\"Karpenter Processing\"] Pods[\"Pending Pods\"] --\u003e BinPack[\"Bin-Packing\u003cbr/\u003eAlgorithm\"] BinPack --\u003e Filter[\"Filter Instance\u003cbr/\u003eTypes\"] Filter --\u003e Overlay[\"Apply NodeOverlay\u003cbr/\u003ePrice Adjustments\"] Overlay --\u003e Fleet[\"CreateFleet\u003cbr/\u003eRequest\"] end Filter --\u003e|\"If all ARM64\u003cbr/\u003efiltered out\"| NoInfluence[\"NodeOverlay\u003cbr/\u003ecannot help\"] style NoInfluence fill:#FFB6C1,color:#000 The Bin-Packing Algorithm Karpenter uses a First-Fit Decreasing (FFD) bin-packing algorithm to minimize the number of nodes needed for pending pods.\nHow It Works Sort pods by resource requirements (largest first) For each pod, try to fit it into an existing “virtual node” If no existing node can fit the pod, create a new virtual node Select the smallest instance type that can satisfy each virtual node’s aggregate requirements Code: Pod Sorting Pods are sorted by CPU and memory in descending order before scheduling begins. This is the first step of the FFD algorithm.\nFrom queue.go:37-43:\n// NewQueue constructs a new queue given the input pods, sorting them to optimize for bin-packing into nodes. func NewQueue(pods []*v1.Pod, podData map[types.UID]*PodData) *Queue { sort.Slice(pods, byCPUAndMemoryDescending(pods, podData)) return \u0026Queue{ pods: pods, lastLen: map[types.UID]int{}, } } The sorting function prioritizes CPU, then memory. From queue.go:72-108:\nfunc byCPUAndMemoryDescending(pods []*v1.Pod, podData map[types.UID]*PodData) func(i int, j int) bool { return func(i, j int) bool { lhs := podData[lhsPod.UID].Requests rhs := podData[rhsPod.UID].Requests cpuCmp := resources.Cmp(lhs[v1.ResourceCPU], rhs[v1.ResourceCPU]) if cpuCmp \u003c 0 { // LHS has less CPU, so it should be sorted after return false } else if cpuCmp \u003e 0 { return true } // ... memory comparison follows } } Code: Scheduling Loop The main scheduling loop in scheduler.go:381-436 processes pods in order:\nfunc (s *Scheduler) Solve(ctx context.Context, pods []*corev1.Pod) (Results, error) { // ... q := NewQueue(pods, s.cachedPodData) for { pod, ok := q.Pop() if !ok { break } if err := s.trySchedule(ctx, pod.DeepCopy()); err != nil { // ... handle error, relax preferences, retry } } // ... } Code: Adding Pods to NodeClaims When adding a pod, Karpenter tries existing nodes first, then in-flight NodeClaims, then creates new ones. From scheduler.go:493-518:\nfunc (s *Scheduler) add(ctx context.Context, pod *corev1.Pod) error { // first try to schedule against an in-flight real node if err := s.addToExistingNode(ctx, pod); err == nil { return nil } // Sort NodeClaims by number of pods (fewer pods = more room) sort.Slice(s.newNodeClaims, func(a, b int) bool { return len(s.newNodeClaims[a].Pods) \u003c len(s.newNodeClaims[b].Pods) }) // Pick existing node that we are about to create if err := s.addToInflightNode(ctx, pod); err == nil { return nil } // Create a new NodeClaim return s.addToNewNodeClaim(ctx, pod) } Example: Bin-Packing in Action Consider 10 pending pods, each requesting 12 vCPUs:\nTotal CPU needed: 10 pods x 12 vCPU = 120 vCPU Karpenter’s options:\nOption A: 2 nodes x 64 vCPU = 128 vCPU capacity (8 vCPU wasted) Option B: 1 node x 128 vCPU = 128 vCPU capacity (8 vCPU wasted) The algorithm prefers Option B because it minimizes node count, even though the total capacity is the same.\nHow Bin-Packing Filters Instance Types When Karpenter determines that a single node with 120+ vCPU is optimal, it filters the instance type list to only include types that can satisfy this requirement.\nCode: Instance Type Filtering The filtering happens in nodeclaim.go:383-451:\nfunc filterInstanceTypesByRequirements( instanceTypes []*cloudprovider.InstanceType, requirements scheduling.Requirements, podRequests, daemonRequests, totalRequests corev1.ResourceList, relaxMinValues bool, ) (cloudprovider.InstanceTypes, map[string]int, error) { remaining := cloudprovider.InstanceTypes{} for _, it := range instanceTypes { itCompat := compatible(it, requirements) itFits := fits(it, totalRequests) // \u003c-- This checks if instance can fit total resources itHasOffering := false for _, of := range it.Offerings { if of.Available \u0026\u0026 requirements.IsCompatible(of.Requirements, ...) { itHasOffering = true break } } // Only keep instance types that meet ALL criteria if itCompat \u0026\u0026 itFits \u0026\u0026 itHasOffering { remaining = append(remaining, it) } } // ... } The fits() function at nodeclaim.go:457-459 checks if an instance type can accommodate the total resource requests:\nfunc fits(instanceType *cloudprovider.InstanceType, requests corev1.ResourceList) bool { return resources.Fits(requests, instanceType.Allocatable()) } Visualization flowchart TB subgraph Before[\"Before Bin-Packing Filter\"] All[\"All Instance Types\"] ARM64_1[\"arm64: 24xlarge (96 vCPU)\"] ARM64_2[\"arm64: 48xlarge (192 vCPU)\"] x86_1[\"x86: 24xlarge (96 vCPU)\"] x86_2[\"x86: 32xlarge (128 vCPU)\"] x86_3[\"x86: 48xlarge (192 vCPU)\"] All --\u003e ARM64_1 \u0026 ARM64_2 \u0026 x86_1 \u0026 x86_2 \u0026 x86_3 end subgraph Filter[\"Bin-Pack Requirement: \u003e= 120 vCPU\"] Check{\"fits(it, totalRequests)\u003cbr/\u003eCan fit 120 vCPU?\"} end subgraph After[\"After Bin-Packing Filter\"] Remaining[\"Eligible Instance Types\"] ARM64_2_ok[\"arm64: 48xlarge (192 vCPU)\"] x86_2_ok[\"x86: 32xlarge (128 vCPU)\"] x86_3_ok[\"x86: 48xlarge (192 vCPU)\"] Remaining --\u003e ARM64_2_ok \u0026 x86_2_ok \u0026 x86_3_ok end Before --\u003e Filter --\u003e After style ARM64_1 fill:#FFB6C1,color:#000 style x86_1 fill:#FFB6C1,color:#000 In this example, both architectures still have eligible instances (48xlarge), so NodeOverlay can influence the selection.\nThe ARM64 Size Gap Problem AWS Graviton (ARM64) instances have a size gap that x86 instances don’t have:\nSize ARM64 (Graviton) x86 (Intel/AMD) 24xlarge 96 vCPU 96 vCPU 32xlarge Does not exist 128 vCPU 48xlarge 192 vCPU 192 vCPU This gap creates a range (97-128 vCPU) where only x86 instances are available.\nThe Problem Scenario If your NodePool has maxVcpu: 128 and bin-packing requires 100+ vCPU:\nflowchart TB subgraph NodePool[\"NodePool Configuration\"] Config[\"maxVcpu: 128\"] end subgraph Available[\"Available Instance Types\"] ARM64[\"ARM64 Options\"] x86[\"x86 Options\"] ARM64_24[\"24xlarge: 96 vCPU\"] ARM64_48[\"48xlarge: 192 vCPU (exceeds maxVcpu)\"] x86_24[\"24xlarge: 96 vCPU\"] x86_32[\"32xlarge: 128 vCPU\"] ARM64 --\u003e ARM64_24 \u0026 ARM64_48 x86 --\u003e x86_24 \u0026 x86_32 end subgraph BinPack[\"Bin-Packing: Need 100 vCPU\"] Need[\"fits(it, 100 vCPU)\u003cbr/\u003eMinimum: 100 vCPU\"] end subgraph Result[\"Eligible After Filtering\"] Only[\"Only x86 32xlarge qualifies\"] end NodePool --\u003e Available Available --\u003e BinPack BinPack --\u003e Result style ARM64_24 fill:#FFB6C1,color:#000 style ARM64_48 fill:#FFB6C1,color:#000 style x86_24 fill:#FFB6C1,color:#000 style Only fill:#FFB6C1,color:#000 Result: ARM64 is completely filtered out. NodeOverlay’s -50% price adjustment on ARM64 has no effect because there are no ARM64 candidates.\nWhen NodeOverlay Cannot Help NodeOverlay adjusts the priority of instance types in the CreateFleet request. It cannot:\nAdd instance types that were filtered out by bin-packing Change NodePool requirements (like maxVcpu) Override Karpenter’s bin-packing decisions The Decision Flow flowchart TB subgraph Phase1[\"Phase 1: Karpenter Filtering\"] direction TB P1_1[\"NodePool requirements filter\"] P1_2[\"Bin-packing: fits(it, totalRequests)\"] P1_3[\"AMI compatibility filter\"] P1_1 --\u003e P1_2 --\u003e P1_3 end subgraph Phase2[\"Phase 2: NodeOverlay Influence\"] direction TB P2_1[\"Apply price adjustments\u003cbr/\u003e(AdjustedPrice())\"] P2_2[\"Set Priority values\"] P2_3[\"Choose allocation strategy\"] P2_1 --\u003e P2_2 --\u003e P2_3 end subgraph Phase3[\"Phase 3: AWS Selection\"] direction TB P3_1[\"Check spot capacity\"] P3_2[\"Apply allocation strategy\"] P3_3[\"Select instance\"] P3_1 --\u003e P3_2 --\u003e P3_3 end Phase1 --\u003e|\"Filtered list\"| Phase2 Phase2 --\u003e|\"CreateFleet request\"| Phase3 Note1[\"NodeOverlay can only\u003cbr/\u003einfluence instances that\u003cbr/\u003esurvive Phase 1\"] Phase2 -.-\u003e Note1 style Note1 fill:#FFFACD,color:#000 Diagnosing the Issue Symptom: x86 Selected Despite ARM64 Price Preference If you’ve configured a NodeOverlay to prefer ARM64 but are still seeing x86 instances:\nCheck the NodeClaim requirements\nkubectl get nodeclaim \u003cname\u003e -o yaml | grep -A 50 requirements Look for the instance type list. If only x86 types are listed, bin-packing has already filtered out ARM64.\nCheck CloudTrail CreateFleet requests\nLook at the LaunchTemplateConfigs in the request:\nTwo configs (ARM64 AMI + x86 AMI) = NodeOverlay can influence One config (x86 AMI only) = ARM64 was filtered out before NodeOverlay Check the aggregate CPU requirements\nSum up the CPU requests of pods that triggered the NodeClaim. If it exceeds the ARM64 size threshold (e.g., 96 vCPU for 24xlarge), that’s likely the cause.\nExample: Identifying the Problem in NodeClaim # Get the NodeClaim kubectl get nodeclaim example-abc123 -o yaml spec: requirements: - key: node.kubernetes.io/instance-type operator: In values: - m7a.32xlarge # 128 vCPU, x86 only - m7i.32xlarge # 128 vCPU, x86 only - r7a.32xlarge # 128 vCPU, x86 only # Notice: No ARM64 instances listed! This NodeClaim has already been filtered to only include 32xlarge x86 instances.\nExample: CloudTrail Request Analysis With NodeOverlay working (both architectures present):\n{ \"LaunchTemplateConfigs\": [ { \"LaunchTemplateSpecification\": { \"LaunchTemplateName\": \"arm64-template\" }, \"Overrides\": [ { \"InstanceType\": \"m8g.24xlarge\", \"Priority\": 0.635 } ] }, { \"LaunchTemplateSpecification\": { \"LaunchTemplateName\": \"x86-template\" }, \"Overrides\": [ { \"InstanceType\": \"m7i.24xlarge\", \"Priority\": 0.78 } ] } ] } Without ARM64 (bin-packing filtered it out):\n{ \"LaunchTemplateConfigs\": [ { \"LaunchTemplateSpecification\": { \"LaunchTemplateName\": \"x86-template\" }, \"Overrides\": [ { \"InstanceType\": \"m7i.32xlarge\", \"Priority\": 0.95 } ] } ] } Solutions and Workarounds Solution 1: Increase maxVcpu to Include Larger ARM64 Sizes If your NodePool has maxVcpu: 128, increase it to 192 to allow Graviton 48xlarge:\nrequirements: - key: karpenter.k8s.aws/instance-cpu operator: Lt values: - \"193\" # Allows up to 192 vCPU (48xlarge) Trade-off: Larger nodes mean more pods per node, which may affect blast radius during node failures.\nSolution 2: Reduce maxVcpu to Exclude x86-Only Sizes Set maxVcpu: 96 to prevent bin-packing from choosing 32xlarge:\nrequirements: - key: karpenter.k8s.aws/instance-cpu operator: Lt values: - \"97\" # Max 96 vCPU (24xlarge) Trade-off: Karpenter may create more nodes to fit the same workload.\nSolution 3: Explicitly Exclude 32xlarge Sizes requirements: - key: karpenter.k8s.aws/instance-size operator: NotIn values: - 32xlarge Trade-off: Same as Solution 2 – more nodes may be created.\nSolution 4: Force Architecture in NodePool If ARM64 is strongly preferred, constrain the NodePool:\nrequirements: - key: kubernetes.io/arch operator: In values: - arm64 Trade-off: No x86 fallback if ARM64 spot capacity is unavailable.\nSummary Stage What Happens Code Location Can NodeOverlay Influence? Pod Sorting Sort by CPU/memory descending queue.go:37-43 No NodePool Requirements Filter by CPU, memory, family, etc. scheduler.go:144-147 No Bin-Packing fits(it, totalRequests) nodeclaim.go:457-459 No AMI Mapping Group instance types by architecture resolver.go:145-196 No Price Adjustment Apply NodeOverlay adjustments types.go:369-384 Yes CreateFleet AWS selects from eligible instances instance.go:456-486 Yes (via Priority) Key Takeaways:\nNodeOverlay influences selection among eligible candidates, not the filtering process The bin-packing fits() check happens before NodeOverlay can influence selection The ARM64 size gap (no 32xlarge Graviton) can eliminate ARM64 from consideration Check NodeClaim requirements and CloudTrail to diagnose unexpected selections Adjust maxVcpu or exclude specific sizes to ensure ARM64 remains eligible ","categories":"","description":"How Karpenter's bin-packing algorithm can affect and sometimes bypass NodeOverlay price adjustments.","excerpt":"How Karpenter's bin-packing algorithm can affect and sometimes bypass …","ref":"/veneer/docs/concepts/binpacking/","tags":"","title":"Bin-Packing and NodeOverlay"},{"body":"The Veneer Helm chart deploys the controller to a Kubernetes cluster. This page documents all available Helm values.\nInstallation helm install veneer veneer/veneer \\ --namespace veneer-system \\ --create-namespace \\ -f values.yaml Values Reference Replica and Image Value Default Description replicaCount 2 Number of controller replicas (leader election handles HA) image.repository ghcr.io/nextdoor/veneer Container image repository image.pullPolicy IfNotPresent Image pull policy image.tag \"\" Image tag (defaults to chart appVersion) imagePullSecrets [] Image pull secrets for private registries Naming Value Default Description nameOverride \"\" Override the name of the chart fullnameOverride \"\" Override the full name of the release Service Account Value Default Description serviceAccount.create true Create a service account serviceAccount.automount true Automatically mount API credentials serviceAccount.annotations {} Annotations (e.g., eks.amazonaws.com/role-arn for IRSA) serviceAccount.name \"\" Service account name (auto-generated if empty) Pod Configuration Value Default Description podAnnotations {} Annotations to add to the pod podLabels {} Labels to add to the pod Security Context The chart enforces a restrictive security posture by default:\nValue Default Description podSecurityContext.runAsNonRoot true Run as non-root user podSecurityContext.runAsUser 65532 User ID podSecurityContext.fsGroup 65532 Filesystem group ID podSecurityContext.seccompProfile.type RuntimeDefault Seccomp profile securityContext.allowPrivilegeEscalation false Prevent privilege escalation securityContext.capabilities.drop [\"ALL\"] Drop all capabilities securityContext.readOnlyRootFilesystem true Read-only root filesystem securityContext.runAsNonRoot true Run as non-root securityContext.runAsUser 65532 User ID securityContext.seccompProfile.type RuntimeDefault Seccomp profile Controller Configuration The config section is passed directly to Veneer’s config.yaml. See the Configuration Reference for full details on each option, including validation rules and environment variable overrides.\nValue Default Description config.prometheusUrl \"http://prometheus:9090\" Prometheus URL for Lumina metrics config.logLevel \"info\" Log level (debug, info, warn, error) config.metricsBindAddress \":8080\" Metrics endpoint bind address config.healthProbeBindAddress \":8081\" Health probe bind address config.aws.accountId \"123456789012\" AWS account ID (required, change this) config.aws.region \"us-west-2\" AWS region (required) config.overlays.utilizationThreshold 95.0 SP utilization threshold for overlay deletion config.overlays.weights.reservedInstance 30 RI overlay weight config.overlays.weights.ec2InstanceSavingsPlan 20 EC2 Instance SP overlay weight config.overlays.weights.computeSavingsPlan 10 Compute SP overlay weight config.overlays.naming.reservedInstancePrefix \"cost-aware-ri\" RI overlay name prefix config.overlays.naming.ec2InstanceSavingsPlanPrefix \"cost-aware-ec2-sp\" EC2 Instance SP overlay name prefix config.overlays.naming.computeSavingsPlanPrefix \"cost-aware-compute-sp\" Compute SP overlay name prefix Controller Manager Value Default Description controllerManager.leaderElection.enabled true Enable leader election for HA controllerManager.extraArgs [] Extra CLI arguments for the controller Metrics Service Value Default Description metricsService.type ClusterIP Service type metricsService.port 8080 Service port metricsService.annotations {} Service annotations Resources Value Default Description resources.limits.cpu \"1\" CPU limit resources.limits.memory 512Mi Memory limit resources.requests.cpu 200m CPU request resources.requests.memory 128Mi Memory request Health Probes Value Default Description livenessProbe.httpGet.path /healthz Liveness probe path livenessProbe.httpGet.port 8081 Liveness probe port livenessProbe.initialDelaySeconds 15 Initial delay livenessProbe.periodSeconds 20 Check interval livenessProbe.timeoutSeconds 1 Timeout livenessProbe.failureThreshold 3 Failures before restart readinessProbe.httpGet.path /readyz Readiness probe path readinessProbe.httpGet.port 8081 Readiness probe port readinessProbe.initialDelaySeconds 5 Initial delay readinessProbe.periodSeconds 10 Check interval readinessProbe.timeoutSeconds 1 Timeout readinessProbe.failureThreshold 3 Failures before unready Volumes Value Default Description volumes [] Additional volumes for the deployment volumeMounts [] Additional volume mounts Scheduling Value Default Description nodeSelector {} Node selector for pod assignment tolerations [] Tolerations for pod assignment affinity {} Affinity rules for pod assignment RBAC Value Default Description rbac.create true Create ClusterRole and ClusterRoleBinding ServiceMonitor For Prometheus Operator integration:\nValue Default Description serviceMonitor.enabled true Create a ServiceMonitor resource serviceMonitor.interval 30s Scrape interval serviceMonitor.scrapeTimeout 10s Scrape timeout serviceMonitor.labels {} Additional labels serviceMonitor.annotations {} Additional annotations serviceMonitor.relabelings [] Relabel configurations serviceMonitor.metricRelabelings [] Metric relabel configurations Lumina Subchart Veneer can optionally deploy Lumina as a subchart:\nValue Default Description lumina.enabled false Enable Lumina as a subchart When enabled, all Lumina chart values can be passed under the lumina key. See Lumina documentation for available values.\nExample: Production Values replicaCount: 2 config: prometheusUrl: \"http://lumina-prometheus.lumina-system.svc:9090\" logLevel: \"info\" aws: accountId: \"123456789012\" region: \"us-west-2\" overlays: utilizationThreshold: 95.0 resources: limits: cpu: \"1\" memory: 512Mi requests: cpu: 200m memory: 128Mi serviceMonitor: enabled: true interval: 30s serviceAccount: annotations: eks.amazonaws.com/role-arn: \"arn:aws:iam::123456789012:role/veneer-controller\" Example: Development Values replicaCount: 1 config: prometheusUrl: \"http://prometheus.prometheus.svc:9090\" logLevel: \"debug\" aws: accountId: \"000000000000\" region: \"us-west-2\" overlays: disabled: true # Dry-run mode controllerManager: leaderElection: enabled: false # Single replica, no need for leader election resources: limits: cpu: 500m memory: 256Mi requests: cpu: 100m memory: 64Mi ","categories":"","description":"Veneer Helm chart values reference.","excerpt":"Veneer Helm chart values reference.","ref":"/veneer/docs/reference/helm-chart/","tags":"","title":"Helm Chart"},{"body":"This section provides detailed reference documentation for every configurable aspect of Veneer, the metrics it exposes, and the custom resources it manages.\nConfiguration – All configuration options (YAML keys, environment variables, CLI flags) with defaults and validation rules. Start here if you need to tune Veneer’s behavior. Metrics – Complete catalog of Prometheus metrics exposed by Veneer, including reconciliation health, decision tracking, overlay lifecycle, and example PromQL queries. Helm Chart – Full Helm values reference for deploying Veneer, including security context defaults, resource recommendations, and example production/development configurations. NodeOverlay CRD – The NodeOverlay custom resource specification: fields, weight system, naming conventions, and example manifests for each overlay type. ","categories":"","description":"Configuration reference, metrics catalog, Helm chart values, and CRD specification.","excerpt":"Configuration reference, metrics catalog, Helm chart values, and CRD …","ref":"/veneer/docs/reference/","tags":"","title":"Reference"},{"body":"Instance preferences allow you to express instance type preferences directly on Karpenter NodePools using annotations. Veneer watches NodePools and generates NodeOverlay resources for each preference, influencing Karpenter’s provisioning decisions.\nNodeOverlays are preferences, not rules. When Veneer creates a NodeOverlay with a price adjustment, it influences but does not guarantee instance selection. See Instance Selection Deep Dive for how AWS makes the final decision.\nAnnotation Format veneer.io/preference.N: \"\u003cmatcher\u003e [\u003cmatcher\u003e...] adjust=[+-]N%\" Where:\nN is a positive integer (1-9 recommended) that determines overlay weight/priority \u003cmatcher\u003e is key=value1,value2 or key!=value or key\u003evalue or key\u003cvalue adjust specifies the price adjustment percentage Example NodePool apiVersion: karpenter.sh/v1 kind: NodePool metadata: name: my-workload annotations: # Prefer c7a/c7g families with 20% discount veneer.io/preference.1: \"karpenter.k8s.aws/instance-family=c7a,c7g adjust=-20%\" # Prefer ARM64 architecture with 30% discount veneer.io/preference.2: \"kubernetes.io/arch=arm64 adjust=-30%\" # Combined matcher: m7g on ARM64 with 40% discount veneer.io/preference.3: \"karpenter.k8s.aws/instance-family=m7g kubernetes.io/arch=arm64 adjust=-40%\" Generated NodeOverlay For preference veneer.io/preference.1 on the NodePool above, Veneer generates:\napiVersion: karpenter.sh/v1alpha1 kind: NodeOverlay metadata: name: pref-my-workload-1 labels: app.kubernetes.io/managed-by: veneer veneer.io/type: preference veneer.io/source-nodepool: my-workload spec: requirements: - key: karpenter.sh/nodepool operator: In values: [\"my-workload\"] - key: karpenter.k8s.aws/instance-family operator: In values: [\"c7a\", \"c7g\"] priceAdjustment: \"-20%\" weight: 1 Supported Labels The following Karpenter and Kubernetes labels can be used in matchers:\nLabel Description Example Values karpenter.k8s.aws/instance-family Instance family c7a, m7g, r6i karpenter.k8s.aws/instance-category Instance category c, m, r, t karpenter.k8s.aws/instance-generation Instance generation number 6, 7, 8 karpenter.k8s.aws/instance-size Instance size large, xlarge, 2xlarge karpenter.k8s.aws/instance-cpu Number of vCPUs 4, 8, 16 karpenter.k8s.aws/instance-cpu-manufacturer CPU manufacturer intel, amd, aws karpenter.k8s.aws/instance-memory Memory in MiB 8192, 16384 kubernetes.io/arch Architecture amd64, arm64 karpenter.sh/capacity-type Capacity type on-demand, spot node.kubernetes.io/instance-type Specific instance type m5.xlarge, c7g.2xlarge Operators Syntax Kubernetes Operator Description Example = In Match any of the values instance-family=c7a,c7g != NotIn Exclude all of the values instance-family!=t3,t3a \u003e Gt Greater than (numeric) instance-cpu\u003e4 \u003c Lt Less than (numeric) instance-cpu\u003c64 Multiple Matchers You can combine multiple matchers in a single preference to create compound requirements. All matchers must match for the overlay to apply:\n# Prefer m7g instances on ARM64 with \u003e= 8 vCPUs veneer.io/preference.1: \"karpenter.k8s.aws/instance-family=m7g kubernetes.io/arch=arm64 karpenter.k8s.aws/instance-cpu\u003e7 adjust=-40%\" This generates a NodeOverlay with three requirements (plus the NodePool selector), all of which must be satisfied for the price adjustment to apply.\nWeight and Priority The number N in veneer.io/preference.N becomes the overlay weight:\nLower numbers = lower weight (lower priority) Higher numbers = higher weight (higher priority) Weight Hierarchy Overlay Type Default Weight Priority Reserved Instance overlays 30 Highest (most specific) EC2 Instance SP overlays 20 Medium Compute SP overlays 10 Lower Preference overlays N (1-9) Lowest Keep preference numbers below 10 to ensure RI/SP overlays (backed by real AWS capacity data) take precedence over user-defined preferences.\nPreference Lifecycle Event Action Preference annotation added to NodePool Veneer creates a NodeOverlay Preference annotation value changed Veneer updates the NodeOverlay Preference annotation removed Veneer deletes the NodeOverlay NodePool deleted NodeOverlay is garbage collected via owner reference Common Patterns Prefer ARM64 (Graviton) annotations: veneer.io/preference.1: \"kubernetes.io/arch=arm64 adjust=-30%\" Prefer Specific Instance Families annotations: veneer.io/preference.1: \"karpenter.k8s.aws/instance-family=c7g,m7g adjust=-25%\" Prefer Latest Generation annotations: veneer.io/preference.1: \"karpenter.k8s.aws/instance-generation\u003e6 adjust=-15%\" Avoid Small Instances annotations: veneer.io/preference.1: \"karpenter.k8s.aws/instance-cpu\u003e7 adjust=-10%\" Layered Preferences You can stack preferences with increasing specificity and discounts:\nannotations: # Slight preference for ARM64 veneer.io/preference.1: \"kubernetes.io/arch=arm64 adjust=-10%\" # Stronger preference for Graviton 7th gen veneer.io/preference.2: \"karpenter.k8s.aws/instance-generation=7 kubernetes.io/arch=arm64 adjust=-20%\" # Strongest preference for c7g family specifically veneer.io/preference.3: \"karpenter.k8s.aws/instance-family=c7g adjust=-30%\" Disabling Preferences Preference processing can be disabled globally via configuration:\n# config.yaml preferences: enabled: false When disabled, the NodePool reconciler will not generate overlays from veneer.io/preference.N annotations. Existing preference overlays will be cleaned up.\n","categories":"","description":"Define instance type preferences on NodePools using annotations to influence Karpenter provisioning.","excerpt":"Define instance type preferences on NodePools using annotations to …","ref":"/veneer/docs/concepts/preferences/","tags":"","title":"Instance Preferences"},{"body":"NodeOverlay is a Karpenter custom resource (karpenter.sh/v1alpha1) that allows adjusting instance type pricing used during provisioning. Veneer creates and manages NodeOverlay resources to influence which instances Karpenter selects.\nAPI Definition apiVersion: karpenter.sh/v1alpha1 kind: NodeOverlay metadata: name: \u003coverlay-name\u003e labels: app.kubernetes.io/managed-by: veneer veneer.io/type: \u003ccost-aware|preference\u003e veneer.io/source-nodepool: \u003cnodepool-name\u003e # For preference overlays spec: # Requirements that an instance must match for this overlay to apply requirements: - key: \u003clabel-key\u003e operator: \u003cIn|NotIn|Gt|Lt|Exists|DoesNotExist\u003e values: [\u003cvalue1\u003e, \u003cvalue2\u003e, ...] # Price adjustment (percentage string) priceAdjustment: \"\u003cadjustment\u003e\" # Weight for overlay precedence (higher = higher priority) weight: \u003cinteger\u003e Spec Fields spec.requirements A list of node selector requirements that an instance type must match for this overlay’s price adjustment to apply. Uses the same requirement format as Karpenter NodePool requirements.\nField Type Description key string Label key to match (e.g., karpenter.k8s.aws/instance-family) operator string Comparison operator: In, NotIn, Gt, Lt, Exists, DoesNotExist values []string Values to match against All requirements must be satisfied for the overlay to apply to an instance type.\nspec.priceAdjustment A string representing the price adjustment to apply. Veneer uses percentage adjustments:\n\"-30%\" – Reduce the effective price by 30% (makes instance more preferred) \"+20%\" – Increase the effective price by 20% (makes instance less preferred) The adjusted price becomes the Priority value in the AWS CreateFleet API call. Lower Priority = higher preference.\nspec.weight An integer that determines overlay precedence. When multiple overlays match the same instance type, the overlay with the highest weight takes effect.\nWeight System Veneer uses a tiered weight system to ensure the most specific capacity data takes precedence:\nOverlay Type Default Weight Scope Naming Prefix Reserved Instance 30 Instance-type + region specific cost-aware-ri- EC2 Instance Savings Plan 20 Instance-family + region specific cost-aware-ec2-sp- Compute Savings Plan 10 Global (all families, all regions) cost-aware-compute-sp- Preference 1-9 User-defined pref- This hierarchy ensures that when both an RI and a Compute SP apply to the same instance type, the more specific RI overlay (weight 30) takes precedence over the general Compute SP overlay (weight 10).\nNaming Conventions Cost-Aware Overlays Veneer generates overlay names from the capacity type prefix and identifying attributes:\nType Pattern Example Reserved Instance {prefix}-{instance-type}-{region} cost-aware-ri-m5-xlarge-us-west-2 EC2 Instance SP {prefix}-{family}-{region} cost-aware-ec2-sp-m5-us-west-2 Compute SP {prefix}-global cost-aware-compute-sp-global Naming prefixes are configurable via the overlays.naming configuration. See Configuration Reference.\nPreference Overlays Preference overlays use the NodePool name and preference number:\nPattern Example pref-{nodepool-name}-{N} pref-my-workload-1 Labels All Veneer-managed overlays carry these labels:\nLabel Value Description app.kubernetes.io/managed-by veneer Identifies Veneer-managed overlays veneer.io/type cost-aware or preference Overlay source type veneer.io/source-nodepool NodePool name (Preference overlays only) Source NodePool These labels are used for:\nListing all Veneer-managed overlays: kubectl get nodeoverlays -l app.kubernetes.io/managed-by=veneer Filtering by type: kubectl get nodeoverlays -l veneer.io/type=preference Finding overlays for a NodePool: kubectl get nodeoverlays -l veneer.io/source-nodepool=my-workload Examples Cost-Aware: Reserved Instance Overlay Created when Lumina detects active Reserved Instances for m5.xlarge in us-west-2:\napiVersion: karpenter.sh/v1alpha1 kind: NodeOverlay metadata: name: cost-aware-ri-m5-xlarge-us-west-2 labels: app.kubernetes.io/managed-by: veneer veneer.io/type: cost-aware spec: requirements: - key: node.kubernetes.io/instance-type operator: In values: [\"m5.xlarge\"] - key: topology.kubernetes.io/region operator: In values: [\"us-west-2\"] - key: karpenter.sh/capacity-type operator: In values: [\"on-demand\"] priceAdjustment: \"-100%\" weight: 30 Cost-Aware: EC2 Instance Savings Plan Overlay Created when Lumina detects an EC2 Instance Savings Plan covering the m5 family in us-west-2 with remaining capacity:\napiVersion: karpenter.sh/v1alpha1 kind: NodeOverlay metadata: name: cost-aware-ec2-sp-m5-us-west-2 labels: app.kubernetes.io/managed-by: veneer veneer.io/type: cost-aware spec: requirements: - key: karpenter.k8s.aws/instance-family operator: In values: [\"m5\"] - key: topology.kubernetes.io/region operator: In values: [\"us-west-2\"] - key: karpenter.sh/capacity-type operator: In values: [\"on-demand\"] priceAdjustment: \"-30%\" weight: 20 Cost-Aware: Compute Savings Plan Overlay Created when Lumina detects a Compute Savings Plan with remaining capacity:\napiVersion: karpenter.sh/v1alpha1 kind: NodeOverlay metadata: name: cost-aware-compute-sp-global labels: app.kubernetes.io/managed-by: veneer veneer.io/type: cost-aware spec: requirements: - key: karpenter.sh/capacity-type operator: In values: [\"on-demand\"] priceAdjustment: \"-15%\" weight: 10 Preference Overlay Created from a veneer.io/preference.2 annotation on a NodePool:\napiVersion: karpenter.sh/v1alpha1 kind: NodeOverlay metadata: name: pref-my-workload-2 labels: app.kubernetes.io/managed-by: veneer veneer.io/type: preference veneer.io/source-nodepool: my-workload spec: requirements: - key: karpenter.sh/nodepool operator: In values: [\"my-workload\"] - key: kubernetes.io/arch operator: In values: [\"arm64\"] priceAdjustment: \"-30%\" weight: 2 Disabled Mode Overlay When overlays.disabled: true, overlays include an impossible requirement:\napiVersion: karpenter.sh/v1alpha1 kind: NodeOverlay metadata: name: cost-aware-ri-m5-xlarge-us-west-2 spec: requirements: - key: node.kubernetes.io/instance-type operator: In values: [\"m5.xlarge\"] - key: veneer.io/disabled operator: In values: [\"true\"] # No node will ever have this label priceAdjustment: \"-100%\" weight: 30 How NodeOverlay Affects Karpenter When a NodeOverlay exists, Karpenter changes its behavior in two ways:\nPrice Adjustment: The overlay’s priceAdjustment modifies the effective price of matching instance types. This adjusted price is used for sorting and as the Priority value in the CreateFleet API call.\nAllocation Strategy Change: The presence of any NodeOverlay switches the allocation strategy:\nSpot: price-capacity-optimized becomes capacity-optimized-prioritized On-Demand: lowest-price becomes prioritized This ensures AWS uses the Priority values (adjusted prices) when selecting instances.\nSee Instance Selection Deep Dive for the full technical explanation.\n","categories":"","description":"NodeOverlay custom resource specification, weight system, and naming conventions.","excerpt":"NodeOverlay custom resource specification, weight system, and naming …","ref":"/veneer/docs/reference/nodeoverlay/","tags":"","title":"NodeOverlay CRD"},{"body":"Common Issues No NodeOverlays Created Symptom: Veneer is running but no NodeOverlays appear.\nCheck data availability:\n# Check if Lumina data is available kubectl port-forward -n veneer-system svc/veneer-metrics 8080:8080 curl -s http://localhost:8080/metrics | grep veneer_lumina_data_available # Expected: veneer_lumina_data_available 1 If veneer_lumina_data_available is 0:\nVerify Lumina is running: kubectl get pods -n lumina-system Verify Prometheus is scraping Lumina: check Prometheus targets UI Verify the Prometheus URL is correct in Veneer’s configuration Check utilization threshold:\ncurl -s http://localhost:8080/metrics | grep veneer_savings_plan_utilization If utilization is above the configured utilization threshold (default 95%), overlays will not be created because the pre-paid capacity is fully consumed.\nCheck disabled mode:\ncurl -s http://localhost:8080/metrics | grep veneer_config_overlays_disabled # Expected: veneer_config_overlays_disabled 0 If 1, Veneer is in disabled mode. Overlays are created but with an impossible requirement so they never match. See the NodeOverlay CRD reference for details on disabled mode overlays.\nx86 Selected Despite ARM64 Preference Symptom: You configured a preference for ARM64 but Karpenter still provisions x86 instances.\nThis can happen for several reasons:\nBin-packing filtered out ARM64 – If the aggregate CPU requirement falls in the 97-128 vCPU range, only x86 32xlarge instances are available (Graviton has no 32xlarge). See Bin-Packing and NodeOverlay.\nARM64 spot capacity exhausted – Even with lower Priority values, AWS will select x86 if ARM64 spot pools lack capacity. Check CloudTrail for InsufficientInstanceCapacity errors.\nNodeOverlay not applied – Verify the overlay exists and matches the instance types:\nkubectl get nodeoverlays -l veneer.io/type=preference See the NodeOverlay CRD reference for label and requirement details.\nSee the Bin-Packing page for diagnostic steps and solutions.\n“Failed to Query Data Freshness” Errors Symptom: Log errors about Prometheus connectivity.\n# Check Prometheus connectivity curl http://\u003cprometheus-url\u003e:9090/-/healthy # Check Prometheus targets curl -s http://\u003cprometheus-url\u003e:9090/api/v1/targets | jq '.data.activeTargets[] | {job: .labels.job, health: .health}' # Verify Lumina metrics exist curl -s 'http://\u003cprometheus-url\u003e:9090/api/v1/query?query=savings_plan_remaining_capacity' | jq '.data.result | length' If running locally with port-forward:\n# Verify port-forward is active lsof -i:9090 # Re-establish if needed kubectl port-forward -n lumina-system svc/lumina-prometheus 9090:9090 “Context Deadline Exceeded” Errors Symptom: Timeout errors when querying Prometheus.\nCheck that Lumina is running and healthy: kubectl get pods -n lumina-system Verify Prometheus has scraped recent metrics Check Prometheus query performance – some queries may be slow on large datasets Port Already in Use Symptom: Veneer fails to start with a bind error.\n# Find process using the port lsof -ti:8081 | xargs kill -9 # Or change the port in config (see Configuration Reference) healthProbeBindAddress: \":8082\" Port values are configurable via the Configuration reference or Helm chart values.\nOverlays Created But Karpenter Ignores Them Symptom: NodeOverlays exist but provisioning behavior doesn’t change.\nVerify Karpenter supports NodeOverlay:\nkubectl get crd nodeoverlays.karpenter.sh Check overlay requirements match instance types: The requirements in the overlay must match instances that Karpenter is considering. Verify with:\nkubectl get nodeoverlay \u003cname\u003e -o yaml Check allocation strategy in CloudTrail: Look for capacity-optimized-prioritized (spot) or prioritized (on-demand) in CreateFleet requests. If you see price-capacity-optimized or lowest-price, NodeOverlay is not being applied.\n“No Matching Capacity” Warnings Symptom: Veneer can’t match Savings Plans utilization with capacity data.\nCheck that Lumina is exposing both utilization and capacity metrics Verify ARNs match between metrics Enable debug logging to see actual query results: logLevel: \"debug\" Data Freshness Veneer checks Lumina data freshness before each reconciliation. If data is stale (older than expected), Veneer skips the reconciliation cycle to avoid making decisions based on outdated information.\nMonitor freshness:\ncurl -s http://localhost:8080/metrics | grep veneer_lumina_data_freshness_seconds The veneer_lumina_data_available metric reports whether data is fresh enough to act on. See the Metrics reference for the full list of available metrics.\nCommon causes of stale data:\nLumina controller is not running or is unhealthy Prometheus is not scraping Lumina Network connectivity issues between Veneer and Prometheus Debugging with Logs Enable Debug Logging # config.yaml logLevel: \"debug\" Or via environment variable:\nexport VENEER_LOG_LEVEL=debug Key Log Messages Log Message Meaning Starting metrics reconciler Controller started successfully Reconciliation complete A reconciliation cycle finished Lumina data is stale Data freshness check failed, skipping cycle Creating NodeOverlay An overlay is being created Deleting NodeOverlay An overlay is being removed SP utilization at/above threshold SP is fully utilized, no overlay needed SP utilization below threshold SP has remaining capacity, overlay created Useful kubectl Commands # View Veneer logs kubectl logs -n veneer-system -l app.kubernetes.io/name=veneer --tail=100 # Follow logs in real-time kubectl logs -n veneer-system -l app.kubernetes.io/name=veneer -f # List all Veneer-managed overlays kubectl get nodeoverlays -l app.kubernetes.io/managed-by=veneer # Describe a specific overlay kubectl describe nodeoverlay cost-aware-ec2-sp-m5-us-west-2 # Check Veneer metrics kubectl port-forward -n veneer-system svc/veneer-metrics 8080:8080 curl -s http://localhost:8080/metrics | grep veneer_ ","categories":"","description":"Common issues, debugging steps, and data freshness for Veneer.","excerpt":"Common issues, debugging steps, and data freshness for Veneer.","ref":"/veneer/docs/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":"Prerequisites Required Tools make: Build automation (usually pre-installed on macOS/Linux) curl: For downloading Go (usually pre-installed) kubectl: Kubernetes CLI configured with access to a cluster git: Version control Go is installed automatically by make targets – no manual installation required.\nOptional Tools kind: For local Kubernetes testing golangci-lint: For full linting (currently disabled due to Go 1.24 compatibility) Quick Start # Clone the repository git clone https://github.com/Nextdoor/veneer.git cd veneer # Build (automatically installs Go and downloads dependencies) make build # Run tests make test The first run of make build or make test will:\nDownload and install Go into ./bin/go/ (version from go.mod) Download Go module dependencies Build or test the project Running Locally Against a Real Cluster Configure Kubernetes access:\nkubectl config current-context kubectl get nodes kubectl get pods -n lumina-system Port-forward to Prometheus (keep running in a separate terminal):\nkubectl port-forward -n lumina-system svc/lumina-prometheus 9090:9090 Run the controller:\nmake run Verify:\ncurl http://localhost:8081/healthz curl http://localhost:8080/metrics Local Dev Environment (Fully Mocked) For development without real AWS infrastructure, Veneer provides a fully mocked local environment using Kind, LocalStack, and Lumina with test data.\nWhat’s Included Component Description Kind cluster Local Kubernetes cluster LocalStack Mocked AWS EC2/STS APIs with seeded instances Lumina Controller using test data for Savings Plans Prometheus Scrapes Lumina metrics Mock nodes K8s nodes correlated to LocalStack EC2 instances Setup # One command to bring up everything make dev-env-up This creates:\nA Kind cluster named veneer 16 EC2 instances in LocalStack (m5.xlarge, c5.large, r5.large, m5.large) 3 Savings Plans with test rates 2 Reserved Instances Mock Kubernetes nodes with matching providerIDs Available Make Targets Target Description make dev-env-up Deploy the full dev environment make dev-env-down Tear down the dev environment make dev-env-restart Restart (down + up) make dev-env-status Show status of all components make dev-env-logs Follow logs from all components make kind-create Create Kind cluster only make kind-delete Delete Kind cluster Running Against Local Environment # Terminal 1: Start port-forward to Prometheus kubectl port-forward -n prometheus svc/prometheus 9090:9090 # Terminal 2: Run Veneer make run Verifying the Environment # Check all pods are running make dev-env-status # Check mock nodes exist with providerIDs kubectl get nodes -l veneer.io/mock-node=true \\ -o custom-columns=NAME:.metadata.name,TYPE:.metadata.labels.node\\\\.kubernetes\\\\.io/instance-type,PROVIDER-ID:.spec.providerID # Query Prometheus for Lumina metrics kubectl port-forward -n prometheus svc/prometheus 9090:9090 \u0026 curl -s 'http://localhost:9090/api/v1/query?query=savings_plan_remaining_capacity' | jq '.data.result' Available Test Metrics Metric Description Example Values ec2_instance_hourly_cost Per-instance hourly cost $0.048 (m5.xlarge) savings_plan_remaining_capacity Unused SP capacity $9.86, $4.71, $2.84 savings_plan_utilization_percent SP utilization rate 1.4%, 5.8%, 5.3% savings_plan_hourly_commitment SP hourly commitment $10, $5, $3 ec2_reserved_instance RI count by type 2 (t2.small) EC2 Test Instances Instance Type Count Purpose m5.xlarge 6 Tests EC2 Instance SP rates c5.large 4 Tests EC2 Instance SP rates r5.large 4 Tests Compute SP rates m5.large (spot) 2 Tests spot pricing Customizing Test Data Add more EC2 instances: Edit hack/dev-env/localstack/init-configmap.yaml Change Savings Plans: Edit hack/dev-env/lumina/configmap.yaml under testData.savingsPlans Modify SP rates: Edit hack/dev-env/lumina/configmap.yaml under testData.savingsPlanRates After changes:\nmake dev-env-restart Cleaning Up # Remove dev environment but keep Kind cluster make dev-env-down # Remove everything including Kind cluster make dev-env-down make kind-delete Testing Unit Tests # Run all tests make test # View coverage report make cover # Run tests for a specific package ./bin/go/bin/go test ./pkg/overlay/... -v # Run a specific test ./bin/go/bin/go test ./pkg/overlay/... -run TestAnalyzeComputeSavingsPlan -v # Run with race detection ./bin/go/bin/go test -race ./pkg/... ./cmd/... ./internal/... Integration Tests Integration tests validate end-to-end behavior with mock Prometheus servers and K8s API interactions:\n# Run all integration tests ./bin/go/bin/go test ./pkg/overlay/... -run Integration -v # Run specific integration test ./bin/go/bin/go test ./pkg/overlay/... -run TestDecisionEngineIntegration -v Integration tests use mock servers and test fixtures, so they don’t require a real cluster.\nTest-Driven Development We encourage TDD:\nWrite a failing test that describes the desired behavior Implement the feature to make the test pass Refactor if needed while keeping tests green Configuration Local Development Config Create config.local.yaml from config.example.yaml:\nprometheusUrl: \"http://localhost:9090\" logLevel: \"debug\" metricsBindAddress: \":8080\" healthProbeBindAddress: \":8081\" aws: accountId: \"123456789012\" region: \"us-west-2\" overlays: disabled: false Environment Variable Overrides export VENEER_PROMETHEUS_URL=\"http://prometheus.example.com:9090\" export VENEER_LOG_LEVEL=\"info\" export VENEER_OVERLAY_DISABLED=\"true\" make run CLI Flags ./bin/manager --config=config.local.yaml --overlay-disabled ./bin/manager --help Project Structure veneer/ ├── cmd/ │ └── main.go # Controller entrypoint ├── pkg/ │ ├── config/ # Configuration management │ ├── metrics/ # Prometheus metrics instrumentation │ ├── overlay/ # Decision engine and overlay generation │ ├── preference/ # NodePool preference parsing and overlay generation │ ├── prometheus/ # Prometheus client for Lumina metrics │ └── reconciler/ # Controller reconciliation loops ├── internal/ │ └── testutil/ # Test fixtures and mock servers ├── test/ │ └── e2e/ # End-to-end tests (require real cluster) ├── charts/ # Helm charts for deployment ├── hack/ │ └── dev-env/ # Local development environment scripts ├── config.local.yaml # Local development configuration └── config.example.yaml # Example configuration Building # Build binary make build # Binary at: bin/manager # Run directly ./bin/manager --config=config.local.yaml # Build Docker image make docker-build # Push to registry make docker-push Contributing Development Workflow Create a feature branch: git checkout -b feature/your-feature-name Make changes following the project guidelines Run tests: make test Run linting: make lint Commit with conventional commits: git commit -m \"feat(component): description\" Push: git push origin feature/your-feature-name Open a Pull Request in draft mode Conventional Commit Format \u003ctype\u003e(component): \u003cdescription\u003e Valid types: feat, fix, docs, test, refactor, chore, ci\nExamples:\nfeat(overlay): add support for cross-account Savings Plans fix(prometheus): handle connection timeouts gracefully test(decision): add aggregation test cases Pre-Commit Checklist Before every commit:\n# 1. Lint the code make lint # 2. Run all tests make test # 3. If both pass, commit git add \u003cfiles\u003e git commit -m \"your message\" Code Review Checklist No internal references or hardcoded data Comprehensive test coverage for new functionality All tests pass locally make lint passes Code follows project conventions Documentation updated (if applicable) Commit messages follow conventional format Debugging Enable Debug Logging # config.local.yaml logLevel: \"debug\" Controller-Runtime Debug Logging ./bin/go/bin/go run ./cmd/main.go \\ --config=config.local.yaml \\ --zap-log-level=debug \\ --zap-devel=true Performance Profiling # Goroutine dump curl http://localhost:8080/debug/pprof/goroutine # Memory profile curl http://localhost:8080/debug/pprof/heap \u003e heap.prof go tool pprof heap.prof Troubleshooting Development Issues “No Such File or Directory” for config.local.yaml cp config.example.yaml config.local.yaml # Edit prometheusUrl to use localhost:9090 Tests Fail with “Connection Refused” Integration tests use mock servers. If you see connection errors:\nEnsure you’re running integration tests, not E2E tests Check that no firewall blocks localhost connections Verify no other process uses the ports Building on Apple Silicon (M1/M2) # Build with explicit architecture GOARCH=arm64 make build # Or use Rosetta for x86_64 arch -x86_64 make build Pods Stuck in Pending (Dev Environment) kubectl describe pod -n lumina-system # Mock nodes show as Unknown/NotReady - this is expected # Only the control-plane node is real No Metrics in Prometheus (Dev Environment) # Check Prometheus targets kubectl port-forward -n prometheus svc/prometheus 9090:9090 \u0026 curl -s 'http://localhost:9090/api/v1/targets' | jq '.data.activeTargets[] | {job: .labels.job, health: .health}' # Check Lumina logs kubectl logs -n lumina-system deployment/lumina-controller --tail=50 ","categories":"","description":"Local development setup, testing, and contributing to Veneer.","excerpt":"Local development setup, testing, and contributing to Veneer.","ref":"/veneer/docs/development/","tags":"","title":"Development"},{"body":" Veneer Cost-Aware Karpenter Provisioning\nGet Started View on GitHub Veneer is a Kubernetes controller that optimizes Karpenter provisioning decisions using real-time cost data. It manages NodeOverlay resources based on AWS Reserved Instance and Savings Plans data from Lumina — steering Karpenter toward the most cost-effective instance types.\nCost-Aware Scheduling Automatically creates and manages Karpenter NodeOverlay resources to prioritize instance types covered by Reserved Instances and Savings Plans.\nReal-Time Optimization Continuously reconciles with Lumina’s cost metrics to keep provisioning decisions aligned with your current RI/SP capacity and utilization.\nSeamless Karpenter Integration Works alongside Karpenter’s existing bin-packing and scheduling logic. Veneer influences instance selection without replacing Karpenter’s core functionality.\nQuick Start helm repo add veneer https://oss.nextdoor.com/veneer helm repo update helm install veneer veneer/veneer ","categories":"","description":"","excerpt":" Veneer Cost-Aware Karpenter Provisioning\nGet Started View on GitHub …","ref":"/veneer/","tags":"","title":"Veneer"}]